{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoEncoder2d Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate ``torch_tools.AutoEncoder2d``, we use it to encode and decode MNIST images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 60000 training items.\n",
      "There are 10000 validation items.\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import Compose, ToTensor, RandomRotation\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "training_tfms = Compose([ToTensor(), RandomRotation(180)])\n",
    "valid_tfms = Compose([ToTensor()])\n",
    "\n",
    "\n",
    "train_set = MNIST(\n",
    "    root=\"/home/jim/storage/mnist/\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=training_tfms,\n",
    ")\n",
    "valid_set = MNIST(\n",
    "    root=\"/home/jim/storage/mnist/\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=valid_tfms,\n",
    ")\n",
    "\n",
    "print(f\"There are {len(train_set)} training items.\")\n",
    "print(f\"There are {len(valid_set)} validation items.\")\n",
    "\n",
    "train_loader = DataLoader(train_set, shuffle=True, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_set, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this demo, we want bother with a validation loop during training, and will just use the validation set to do some inference at the end. Let's instantiate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoEncoder2d(\n",
      "  (encoder): Encoder2d(\n",
      "    (0): DoubleConvBlock(\n",
      "      (0): ConvBlock(\n",
      "        (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1)\n",
      "      )\n",
      "      (1): ConvBlock(\n",
      "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1)\n",
      "      )\n",
      "    )\n",
      "    (1): DownBlock(\n",
      "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): DoubleConvBlock(\n",
      "        (0): ConvBlock(\n",
      "          (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): LeakyReLU(negative_slope=0.1)\n",
      "        )\n",
      "        (1): ConvBlock(\n",
      "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): LeakyReLU(negative_slope=0.1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): DownBlock(\n",
      "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): DoubleConvBlock(\n",
      "        (0): ConvBlock(\n",
      "          (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): LeakyReLU(negative_slope=0.1)\n",
      "        )\n",
      "        (1): ConvBlock(\n",
      "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): LeakyReLU(negative_slope=0.1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): Decoder2d(\n",
      "    (0): UpBlock(\n",
      "      (0): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
      "      (1): DoubleConvBlock(\n",
      "        (0): ConvBlock(\n",
      "          (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): LeakyReLU(negative_slope=0.1)\n",
      "        )\n",
      "        (1): ConvBlock(\n",
      "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): LeakyReLU(negative_slope=0.1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): UpBlock(\n",
      "      (0): ConvTranspose2d(128, 128, kernel_size=(2, 2), stride=(2, 2))\n",
      "      (1): DoubleConvBlock(\n",
      "        (0): ConvBlock(\n",
      "          (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): LeakyReLU(negative_slope=0.1)\n",
      "        )\n",
      "        (1): ConvBlock(\n",
      "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): LeakyReLU(negative_slope=0.1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): Conv2d(64, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch.cuda import is_available\n",
    "from torch_tools import AutoEncoder2d\n",
    "\n",
    "DEVICE = \"cuda\" if is_available() else \"cpu\"\n",
    "\n",
    "model = AutoEncoder2d(in_chans=3, out_chans=3, num_layers=3).to(DEVICE)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we set up the optimiser and loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "from torch.nn import L1Loss\n",
    "\n",
    "optimiser = Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "loss_func = L1Loss()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we write our training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Module\n",
    "\n",
    "\n",
    "def train_one_epoch(\n",
    "    model: Module,\n",
    "    data_loader: DataLoader,\n",
    "    optimiser: Adam,\n",
    "    loss_func: L1Loss,\n",
    ") -> float:\n",
    "    \"\"\"Train ``model`` for a single epoch.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : Module\n",
    "        AutoEncoder model.\n",
    "    data_loader : DataLoader\n",
    "        Training data loader.\n",
    "    optimiser : Adam\n",
    "        Adam optimiser.\n",
    "    loss_func : L1Loss\n",
    "        The L1 loss function.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    float\n",
    "        Mean loss per item.\n",
    "\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch, _ in data_loader:\n",
    "\n",
    "        optimiser.zero_grad()\n",
    "\n",
    "        batch = batch.to(DEVICE)\n",
    "\n",
    "        preds = model(batch).tanh()\n",
    "\n",
    "        loss = loss_func(preds, batch)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimiser.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    return running_loss / len(data_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-tools-demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
