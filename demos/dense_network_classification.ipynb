{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dense network demonstrations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started with this demo\n",
    "\n",
    "To get started with this demo, please create an environment based on ``demo-requirements.conda.yaml``. See the ``README`` for instructions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``torch_tools.DenseNetwork`` model can be used for classification or regression.\n",
    "\n",
    "Here we will use it for a classification problem. We need:\n",
    "\n",
    "* Some data:\n",
    "    * For the classification and regression examples we will use some datasets available from scikit-learn.\n",
    "    * For more information, see the [data set's documentation](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html#sklearn.datasets.load_breast_cancer).\n",
    "* Some data loading utilities:\n",
    "    * We need to load the data using a ``Dataset`` and ``DataLoader``.\n",
    "* A model:\n",
    "    * We will instantiate the model shortly.\n",
    "* A loss function.\n",
    "* An optimiser to fit the model with."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification example\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit learn has a bunch of available datasets that we can select an example from. We will use the breast cancer classification dataset. These data include physical measurements from various tumours, and their status as malignant or normal.\n",
    "\n",
    "Let's grab the data from scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "data = load_breast_cancer(as_frame=True).frame\n",
    "\n",
    "valid_split = data.groupby(\"target\").sample(frac=0.2, random_state=123)\n",
    "\n",
    "# The train split contains the items which were not randomly selected for\n",
    "# validation\n",
    "train_split = data.loc[~data.index.isin(valid_split.index)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training and validation datasets are pandas ``DataFrame`` objects, holding various physical characteristics of the tumours, along with their classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     mean radius  mean texture  mean perimeter  mean area  mean smoothness   \n",
      "414       15.130         29.81           96.71      719.5          0.08320  \\\n",
      "432       20.180         19.54          133.80     1250.0          0.11330   \n",
      "449       21.100         20.52          138.10     1384.0          0.09684   \n",
      "260       20.310         27.06          132.90     1288.0          0.10000   \n",
      "492       18.010         20.56          118.40     1007.0          0.10010   \n",
      "..           ...           ...             ...        ...              ...   \n",
      "472       14.920         14.93           96.45      686.9          0.08098   \n",
      "314        8.597         18.60           54.09      221.2          0.10740   \n",
      "421       14.690         13.98           98.22      656.1          0.10310   \n",
      "376       10.570         20.22           70.15      338.3          0.09073   \n",
      "484       15.730         11.28          102.80      747.2          0.10430   \n",
      "\n",
      "     mean compactness  mean concavity  mean concave points  mean symmetry   \n",
      "414           0.04605         0.04686              0.02739         0.1852  \\\n",
      "432           0.14890         0.21330              0.12590         0.1724   \n",
      "449           0.11750         0.15720              0.11550         0.1554   \n",
      "260           0.10880         0.15190              0.09333         0.1814   \n",
      "492           0.12890         0.11700              0.07762         0.2116   \n",
      "..                ...             ...                  ...            ...   \n",
      "472           0.08549         0.05539              0.03221         0.1687   \n",
      "314           0.05847         0.00000              0.00000         0.2163   \n",
      "421           0.18360         0.14500              0.06300         0.2086   \n",
      "376           0.16600         0.22800              0.05941         0.2188   \n",
      "484           0.12990         0.11910              0.06211         0.1784   \n",
      "\n",
      "     mean fractal dimension  ...  worst texture  worst perimeter  worst area   \n",
      "414                 0.05294  ...          36.91           110.10       931.4  \\\n",
      "432                 0.06053  ...          25.07           146.00      1479.0   \n",
      "449                 0.05661  ...          32.07           168.20      2022.0   \n",
      "260                 0.05572  ...          39.16           162.30      1844.0   \n",
      "492                 0.06077  ...          26.06           143.40      1426.0   \n",
      "..                      ...  ...            ...              ...         ...   \n",
      "472                 0.05669  ...          18.22           112.00       906.6   \n",
      "314                 0.07359  ...          22.44            56.65       240.1   \n",
      "421                 0.07406  ...          18.34           114.10       809.2   \n",
      "376                 0.08450  ...          22.82            76.51       351.9   \n",
      "484                 0.06259  ...          14.20           112.50       854.3   \n",
      "\n",
      "     worst smoothness  worst compactness  worst concavity   \n",
      "414            0.1148            0.09866           0.1547  \\\n",
      "432            0.1665            0.29420           0.5308   \n",
      "449            0.1368            0.31010           0.4399   \n",
      "260            0.1522            0.29450           0.3788   \n",
      "492            0.1309            0.23270           0.2544   \n",
      "..                ...                ...              ...   \n",
      "472            0.1065            0.27910           0.3151   \n",
      "314            0.1347            0.07767           0.0000   \n",
      "421            0.1312            0.36350           0.3219   \n",
      "376            0.1143            0.36190           0.6030   \n",
      "484            0.1541            0.29790           0.4004   \n",
      "\n",
      "     worst concave points  worst symmetry  worst fractal dimension  target  \n",
      "414               0.06575          0.3233                  0.06165       0  \n",
      "432               0.21730          0.3032                  0.08075       0  \n",
      "449               0.22800          0.2268                  0.07425       0  \n",
      "260               0.16970          0.3151                  0.07999       0  \n",
      "492               0.14890          0.3251                  0.07625       0  \n",
      "..                    ...             ...                      ...     ...  \n",
      "472               0.11470          0.2688                  0.08273       1  \n",
      "314               0.00000          0.3142                  0.08116       1  \n",
      "421               0.11080          0.2827                  0.09208       1  \n",
      "376               0.14650          0.2597                  0.12000       1  \n",
      "484               0.14520          0.2557                  0.08181       1  \n",
      "\n",
      "[113 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "print(valid_split)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have downloaded the breast cancer dataset from scikit-learn and split the data into a training and validation set. Conventially, one should split the data into a training, validation and testing set, but since this is only a simple example, with a very small dataset, we only use a training and a validation set."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a matter of convenience, let's grab a list of the keys for accessing the data and targets separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mean radius', 'mean texture', 'mean perimeter', 'mean area', 'mean smoothness', 'mean compactness', 'mean concavity', 'mean concave points', 'mean symmetry', 'mean fractal dimension', 'radius error', 'texture error', 'perimeter error', 'area error', 'smoothness error', 'compactness error', 'concavity error', 'concave points error', 'symmetry error', 'fractal dimension error', 'worst radius', 'worst texture', 'worst perimeter', 'worst area', 'worst smoothness', 'worst compactness', 'worst concavity', 'worst concave points', 'worst symmetry', 'worst fractal dimension']\n"
     ]
    }
   ],
   "source": [
    "data_keys = list(filter(lambda x: x != \"target\", data.keys()))\n",
    "target_keys = [\"target\"]\n",
    "\n",
    "print(data_keys)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we create ``DataSet`` objects for the training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Detected that PyTorch and torchvision were compiled with different CUDA versions. PyTorch has CUDA Version=11.8 and torchvision has CUDA Version=11.7. Please reinstall the torchvision that matches your PyTorch install.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtransforms\u001b[39;00m \u001b[39mimport\u001b[39;00m Compose\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch_tools\u001b[39;00m \u001b[39mimport\u001b[39;00m DataSet\n\u001b[1;32m      6\u001b[0m train_set \u001b[39m=\u001b[39m DataSet(\n\u001b[1;32m      7\u001b[0m     inputs\u001b[39m=\u001b[39m\u001b[39mtuple\u001b[39m(train_split[data_keys]\u001b[39m.\u001b[39mto_numpy()),\n\u001b[1;32m      8\u001b[0m     targets\u001b[39m=\u001b[39m\u001b[39mtuple\u001b[39m(train_split[target_keys]\u001b[39m.\u001b[39mto_numpy()),\n\u001b[1;32m      9\u001b[0m     target_tfms\u001b[39m=\u001b[39mCompose([\u001b[39mlambda\u001b[39;00m x: torch\u001b[39m.\u001b[39meye(\u001b[39m2\u001b[39m)[x]\u001b[39m.\u001b[39msqueeze()]),\n\u001b[1;32m     10\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-tools-demo/lib/python3.10/site-packages/torchvision/__init__.py:6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmodulefinder\u001b[39;00m \u001b[39mimport\u001b[39;00m Module\n\u001b[1;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m \u001b[39mimport\u001b[39;00m datasets, io, models, ops, transforms, utils\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mextension\u001b[39;00m \u001b[39mimport\u001b[39;00m _HAS_OPS\n\u001b[1;32m     10\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-tools-demo/lib/python3.10/site-packages/torchvision/datasets/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_optical_flow\u001b[39;00m \u001b[39mimport\u001b[39;00m FlyingChairs, FlyingThings3D, HD1K, KittiFlow, Sintel\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_stereo_matching\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m      3\u001b[0m     CarlaStereo,\n\u001b[1;32m      4\u001b[0m     CREStereo,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m     SintelStereo,\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mcaltech\u001b[39;00m \u001b[39mimport\u001b[39;00m Caltech101, Caltech256\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-tools-demo/lib/python3.10/site-packages/torchvision/datasets/_optical_flow.py:12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mPIL\u001b[39;00m \u001b[39mimport\u001b[39;00m Image\n\u001b[0;32m---> 12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mio\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mimage\u001b[39;00m \u001b[39mimport\u001b[39;00m _read_png_16\n\u001b[1;32m     13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m _read_pfm, verify_str_arg\n\u001b[1;32m     14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mvision\u001b[39;00m \u001b[39mimport\u001b[39;00m VisionDataset\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-tools-demo/lib/python3.10/site-packages/torchvision/io/__init__.py:8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m _log_api_usage_once\n\u001b[1;32m      7\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m----> 8\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_load_gpu_decoder\u001b[39;00m \u001b[39mimport\u001b[39;00m _HAS_GPU_VIDEO_DECODER\n\u001b[1;32m      9\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mModuleNotFoundError\u001b[39;00m:\n\u001b[1;32m     10\u001b[0m     _HAS_GPU_VIDEO_DECODER \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-tools-demo/lib/python3.10/site-packages/torchvision/io/_load_gpu_decoder.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mextension\u001b[39;00m \u001b[39mimport\u001b[39;00m _load_library\n\u001b[1;32m      4\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m      5\u001b[0m     _load_library(\u001b[39m\"\u001b[39m\u001b[39mDecoder\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-tools-demo/lib/python3.10/site-packages/torchvision/extension.py:107\u001b[0m\n\u001b[1;32m    102\u001b[0m             warn(\u001b[39m\"\u001b[39m\u001b[39mLoadLibraryExW is missing in kernel32.dll\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    104\u001b[0m     torch\u001b[39m.\u001b[39mops\u001b[39m.\u001b[39mload_library(lib_path)\n\u001b[0;32m--> 107\u001b[0m _check_cuda_version()\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-tools-demo/lib/python3.10/site-packages/torchvision/extension.py:80\u001b[0m, in \u001b[0;36m_check_cuda_version\u001b[0;34m()\u001b[0m\n\u001b[1;32m     78\u001b[0m     t_minor \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(t_version[\u001b[39m1\u001b[39m])\n\u001b[1;32m     79\u001b[0m     \u001b[39mif\u001b[39;00m t_major \u001b[39m!=\u001b[39m tv_major \u001b[39mor\u001b[39;00m t_minor \u001b[39m!=\u001b[39m tv_minor:\n\u001b[0;32m---> 80\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m     81\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mDetected that PyTorch and torchvision were compiled with different CUDA versions. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     82\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPyTorch has CUDA Version=\u001b[39m\u001b[39m{\u001b[39;00mt_major\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mt_minor\u001b[39m}\u001b[39;00m\u001b[39m and torchvision has \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     83\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCUDA Version=\u001b[39m\u001b[39m{\u001b[39;00mtv_major\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mtv_minor\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     84\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mPlease reinstall the torchvision that matches your PyTorch install.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     85\u001b[0m         )\n\u001b[1;32m     86\u001b[0m \u001b[39mreturn\u001b[39;00m _version\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Detected that PyTorch and torchvision were compiled with different CUDA versions. PyTorch has CUDA Version=11.8 and torchvision has CUDA Version=11.7. Please reinstall the torchvision that matches your PyTorch install."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.transforms import Compose\n",
    "\n",
    "from torch_tools import DataSet\n",
    "\n",
    "train_set = DataSet(\n",
    "    inputs=tuple(train_split[data_keys].to_numpy()),\n",
    "    targets=tuple(train_split[target_keys].to_numpy()),\n",
    "    target_tfms=Compose([lambda x: torch.eye(2)[x].squeeze()]),\n",
    ")\n",
    "\n",
    "valid_set = DataSet(\n",
    "    inputs=tuple(valid_split[data_keys].to_numpy()),\n",
    "    targets=tuple(valid_split[target_keys].to_numpy()),\n",
    "    target_tfms=Compose([lambda x: torch.eye(2)[x].squeeze()]),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``DataSet`` objects are iterables which yield input–target pairs (as ``torch.Tensor``s):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for input_item, target in valid_set:\n",
    "    print(input_item.shape, target.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While PyTorch ``Dataset``s generally return single input-target pairs, we normally supply data to neural networks in mini-batches. The PyTorch way to achieve this is use a ``DataLoader`` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 10\n",
    "\n",
    "train_loader = DataLoader(train_set, shuffle=True, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_set, shuffle=False, batch_size=batch_size)\n",
    "\n",
    "for batch, targets in valid_loader:\n",
    "    print(batch.shape, targets.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can that each input is a ``Tensor`` of shape ``(10, 30)`` because each input has 30 features and we use a batch size of ten. The targets are simply one-hot-encoded vectors giving the binary classification of each input item.\n",
    "\n",
    "Before we train the model. we need to decide whether we want to train the model on the GPU or the CPU. PyTorch has a way of managing this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda import is_available\n",
    "\n",
    "DEVICE = \"cuda\" if is_available() else \"cpu\"\n",
    "print(DEVICE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the data, we need a model, loss function and optimiser. Let's set these things up:\n",
    "\n",
    "- For our choice of model, we can use the ``torch_tools.DenseNetwork`` model.\n",
    "- For the loss function, we choose the (fairly) standard binary-cross-entropy.\n",
    "  - See PyTorch's [BCELoss docs](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html).\n",
    "- For our optimiser, we will use the (again fairly standard) Adam optimiser.\n",
    "  - See PyTorch's [Adam optimiser docs](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import BCELoss\n",
    "from torch.optim import Adam\n",
    "\n",
    "from torch_tools import DenseNetwork\n",
    "\n",
    "\n",
    "loss_func = BCELoss(reduction=\"sum\")\n",
    "\n",
    "model = DenseNetwork(\n",
    "    in_feats=30,\n",
    "    out_feats=2,\n",
    "    hidden_sizes=(16, 8),\n",
    "    input_dropout=0.1,\n",
    "    input_bnorm=True,\n",
    "    hidden_dropout=0.25,\n",
    ").to(DEVICE)\n",
    "print(model)\n",
    "\n",
    "\n",
    "optimiser = Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write functions for training and validating single epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Module\n",
    "from torch import no_grad\n",
    "\n",
    "\n",
    "def train_one_epoch(\n",
    "    model: Module,\n",
    "    loss_func: BCELoss,\n",
    "    optim: Adam,\n",
    "    train_loader: DataLoader,\n",
    "):\n",
    "    \"\"\"Train the model for a single epoch.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : Module\n",
    "        The model we want to train.\n",
    "    loss_func : BCELoss\n",
    "        Binary-cross-entropy loss function.\n",
    "    optim : Adam\n",
    "        Adam optimiser.\n",
    "    train_loader : DataLoader\n",
    "        Training dataloader.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    mean_loss: float\n",
    "        Mean loss per batch.\n",
    "    accuracy : float\n",
    "        Accuracy over the epoch.\n",
    "\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    for batch, targets in train_loader:\n",
    "        # Move the inputs and targets from the cpu to the gpu.\n",
    "        batch, targets = batch.float().to(DEVICE), targets.float().to(DEVICE)\n",
    "        # Zero the gradients.\n",
    "        optim.zero_grad()\n",
    "        # Predict using the batch of inputs.\n",
    "        prediction = model(batch).softmax(dim=1)\n",
    "        # Compute the loss\n",
    "        loss = loss_func(prediction, targets)\n",
    "        # Backpropagate (differentiate the parameters wrt the loss)\n",
    "        loss.backward()\n",
    "        # Step the parameters\n",
    "        optim.step()\n",
    "        # Record the loss\n",
    "        running_loss += loss.item()\n",
    "        # Record the number of correctly classified items\n",
    "        correct += (\n",
    "            (prediction.detach().argmax(dim=1) == targets.argmax(dim=1)).sum().cpu()\n",
    "        )\n",
    "\n",
    "    mean_loss = running_loss / len(train_loader.dataset)\n",
    "    accuracy = correct / len(train_loader.dataset)\n",
    "\n",
    "    return mean_loss, accuracy\n",
    "\n",
    "\n",
    "@no_grad()\n",
    "def validate_one_epoch(\n",
    "    model: Module,\n",
    "    loss_func: BCELoss,\n",
    "    valid_loader: DataLoader,\n",
    "):\n",
    "    \"\"\"Validate single epoch.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : Module\n",
    "        The model to validate.\n",
    "    loss_func : BCELoss\n",
    "        Binary-cross-entropy loss function.\n",
    "    valid_loader : DataLoader\n",
    "        Validation data loader.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Mean loss per item.\n",
    "    accuracy : float\n",
    "        Accuracy over the epoch.\n",
    "\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    for batch, targets in valid_loader:\n",
    "        # Move the inputs and targets from the cpu to the gpu.\n",
    "        batch, targets = batch.float().to(DEVICE), targets.float().to(DEVICE)\n",
    "        # Predict using the batch of inputs\n",
    "        pred = model(batch).softmax(dim=1)\n",
    "        # Calculate the loss\n",
    "        loss = loss_func(pred, targets)\n",
    "        # Record the number of correctly classified items\n",
    "        correct += (pred.argmax(dim=1) == targets.argmax(dim=1)).sum().cpu()\n",
    "        # Record the loss\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    mean_loss = running_loss / len(valid_loader.dataset)\n",
    "    accuracy = correct / len(valid_loader.dataset)\n",
    "\n",
    "    return mean_loss, accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {\n",
    "    \"train_loss\": [],\n",
    "    \"valid_loss\": [],\n",
    "    \"train_acc\": [],\n",
    "    \"valid_acc\": [],\n",
    "}\n",
    "\n",
    "for epoch in range(200):\n",
    "    train_loss, train_acc = train_one_epoch(\n",
    "        model,\n",
    "        loss_func,\n",
    "        optimiser,\n",
    "        train_loader,\n",
    "    )\n",
    "    valid_loss, valid_acc = validate_one_epoch(model, loss_func, valid_loader)\n",
    "\n",
    "    metrics[\"train_loss\"].append(train_loss)\n",
    "    metrics[\"train_acc\"].append(train_acc)\n",
    "    metrics[\"valid_loss\"].append(valid_loss)\n",
    "    metrics[\"valid_acc\"].append(valid_acc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is trained, we can plot the ``metrics`` dictionary we have populated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "figure, axes = plt.subplots(1, 2)\n",
    "axes[0].plot(metrics[\"train_loss\"], label=\"Train\")\n",
    "axes[0].plot(metrics[\"valid_loss\"], label=\"Valid\")\n",
    "\n",
    "\n",
    "axes[1].plot(metrics[\"train_acc\"], label=\"Train\")\n",
    "axes[1].plot(metrics[\"valid_acc\"], label=\"Valid\")\n",
    "\n",
    "axes[0].set_ylim(bottom=0.0)\n",
    "axes[1].set_ylim(bottom=0.5, top=1.0)\n",
    "\n",
    "axes[0].set_ylabel(\"Loss\")\n",
    "axes[1].set_ylabel(\"Classification accuracy\")\n",
    "\n",
    "\n",
    "for axis in axes.ravel():\n",
    "    axis.set_xlabel(\"Epoch\")\n",
    "    axis.set_xlim(left=0.0, right=len(metrics[\"train_acc\"]))\n",
    "    axis.set_aspect(\n",
    "        (axis.get_xlim()[1] - axis.get_xlim()[0])\n",
    "        / (axis.get_ylim()[1] - axis.get_ylim()[0])\n",
    "    )\n",
    "    axis.legend()\n",
    "\n",
    "\n",
    "figure.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With no prior knowledge of this dataset, this seems like reasonable performance. Conventionally, one should now choose the \"best\" model, by examining the validation set, and then apply it to an unseen test set. Generally speaking, the more independent the test set, the better. I.e. in this case, if we had a series of measurements from a different lab or hospital, it would be good to try the trained model out on those data.\n",
    "\n",
    "However, the purpose of this demo is to illustrate how to use the model—and to provide a scientific sanity check that it can be trained—so we skip this testing phase."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-tools-demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ca0431213683a4cf4fa8a04c822ab65ef03858b4260bcbbe63868c9f59e4f0b5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
