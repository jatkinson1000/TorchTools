{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dense neural network model demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This simple model can be used for classification and regression. It can be easily instantiated with custom numbers of layers of arbitrary sizes. The model is composed of dense blocks, which contain\n",
    "\n",
    "Linear -> BatchNorm1d -> Dropout -> LeakyReLU.\n",
    "\n",
    "(Note: the final block is always just a Linear layer with no activation.)\n",
    "\n",
    "The batch-normalisation and dropout are optional (cotrolled at instantation), and the negative slope of the leaky relu can also be easily set (zero if you want regular ReLU).\n",
    "\n",
    "Suppose we want a model which takes `256` input features, produces an ouptut of size `2`, has two hidden layers of size `128` with batch-normalisation and dropout layers of `p=0.25`.\n",
    "\n",
    "Creating such a model in PyTorch is easy enough, but it often results in hacky, ugly code, and the unnecessary duplication of effort. With the `DenseNetwork` class, it's easy---and the model has been tested."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
=======
   "execution_count": 1,
>>>>>>> ee914740ff8d938c267bb5d9d0e5bf55982a9067
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenseNetwork(\n",
      "  (_input_block): InputBlock(\n",
      "    (_fwd_seq): Sequential(\n",
      "      (0): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (1): Dropout(p=0.25, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (_dense_blocks): Sequential(\n",
      "    (0): DenseBlock(\n",
      "      (_fwd_seq): Sequential(\n",
      "        (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Dropout(p=0.25, inplace=False)\n",
      "        (3): LeakyReLU(negative_slope=0.2)\n",
      "      )\n",
      "    )\n",
      "    (1): DenseBlock(\n",
      "      (_fwd_seq): Sequential(\n",
      "        (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Dropout(p=0.25, inplace=False)\n",
      "        (3): LeakyReLU(negative_slope=0.2)\n",
      "      )\n",
      "    )\n",
      "    (2): DenseBlock(\n",
      "      (_fwd_seq): Sequential(\n",
      "        (0): Linear(in_features=128, out_features=2, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch_tools.models import DenseNetwork\n",
    "\n",
    "model = DenseNetwork(in_feats=256,\n",
    "                     out_feats=2,\n",
    "                     hidden_sizes=(128, 128),\n",
    "                     hidden_bnorm=True,\n",
    "                     hidden_dropout=0.25,)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pass some inputs through our model. Note, all PyTorch models are sub classes of `torch.nn.Module`, and take mini-batches of inputs. Let's create a mini-batch of ten inputs containing random noise, and pass them through the model."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 4,
=======
   "execution_count": 2,
>>>>>>> ee914740ff8d938c267bb5d9d0e5bf55982a9067
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "tensor([[0.5467, 0.3058],\n",
      "        [0.4509, 0.4960],\n",
      "        [0.5309, 0.4651],\n",
      "        [0.4521, 0.6927],\n",
      "        [0.5739, 0.4631],\n",
      "        [0.4639, 0.4473],\n",
      "        [0.5026, 0.3298],\n",
      "        [0.6917, 0.4680],\n",
      "        [0.7496, 0.4219],\n",
      "        [0.4446, 0.6263]], grad_fn=<SigmoidBackward0>)\n"
=======
      "tensor([[0.4347, 0.5319],\n",
      "        [0.6300, 0.6061],\n",
      "        [0.4120, 0.4588],\n",
      "        [0.5111, 0.5340],\n",
      "        [0.4604, 0.4618],\n",
      "        [0.4809, 0.5264],\n",
      "        [0.5081, 0.4111],\n",
      "        [0.4598, 0.5679],\n",
      "        [0.5648, 0.4092],\n",
      "        [0.8003, 0.5123]], grad_fn=<SigmoidBackward0>)\n"
>>>>>>> ee914740ff8d938c267bb5d9d0e5bf55982a9067
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.rand(10, 256)\n",
    "\n",
    "prediction = model(x).sigmoid()\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how we (optionally) applied the sigmoid acitvation function to the output, because the final layer doesn't have an activation function."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('torch-tools')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "389daa27a49850dc9127ed4dc2ce151fa3f7d10fd95fdbad85e152cfd906ea76"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
