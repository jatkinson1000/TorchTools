{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ``ConvNet2d`` Demo\n",
    "\n",
    "The ``torch_tools.S``"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying handwritten digits — MNIST\n",
    "\n",
    "The MNIST dataset contains greyscale images of `(28,28)` pipxels containing handwritten digits between zero and nine. It is very easy to access the MNIST data through ``torchvision``—so much so that we don't even need to use the custom ``torch_tools.DataSet`` class.\n",
    "\n",
    "So, let's get some digits!\n",
    "\n",
    "### Demo env\n",
    "Remember: the demo env requirements file—``demo-requirements.conda.yaml``—is different from the standard ``requirements.conda.yaml``. See ``README.md``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 10000\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "from torch import eye\n",
    "from torchvision.transforms import Compose, ToTensor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_set = MNIST(\n",
    "    \"~/mnist_data/\",\n",
    "    download=True,\n",
    "    train=True,\n",
    "    transform=Compose([ToTensor(), lambda x: x.repeat(3, 1, 1)]),\n",
    "    target_transform=Compose([lambda x: eye(10)[x]]),\n",
    ")\n",
    "\n",
    "valid_set = MNIST(\n",
    "    \"~/mnist_data/\",\n",
    "    download=True,\n",
    "    train=False,\n",
    "    transform=Compose([ToTensor(), lambda x: x.repeat(3, 1, 1)]),\n",
    "    target_transform=Compose([lambda x: eye(10)[x]]),\n",
    ")\n",
    "\n",
    "print(len(train_set), len(valid_set))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have done quite a few things here, let's break them down.\n",
    "\n",
    "- The ``transform`` argument:\n",
    "  - we supply a ``torchvision.transfroms.Compose`` object, which simply lets you chain callable objects to modify the images.\n",
    "  - the first thing in the ``Compose`` is a ``torchvision.transforms.ToTensor``, which converts PIL images (or numpy image-like arrays) to ``torch.Tensor``s.\n",
    "  - the second thing is the lambda function, which simply repeats the greyscale ``Tensor`` three times along the channel dimension. We do this because we are going to use a pretrained model which requires three input channels.\n",
    "- The ``target_transform`` argument:\n",
    "  - The target for each image is simply encoded as an integer. ``0`` means the image contains a zero, ``1`` means the image contains a one, etc. We convert these indices to one-hot encoded vectors with the lambda function supplued to ``target_transfrom``.\n",
    "\n",
    "Let's look at a single example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, target = train_set[0]\n",
    "\n",
    "plt.imshow(img.permute(1, 2, 0))\n",
    "plt.title(f\"target: {target}\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example is clearly a five, and the target is a one-hot vector encoding the fact that the ground truth is a five.\n",
    "\n",
    "As before, we now wrap the datasets in ``torch.utils.data.DataLoader``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_set, shuffle=True, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_set, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we need a model, optimiser and a loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleConvNet2d(\n",
      "  (0): Encoder2d(\n",
      "    (0): DoubleConvBlock(\n",
      "      (0): ConvBlock(\n",
      "        (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1)\n",
      "      )\n",
      "      (1): ConvBlock(\n",
      "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1)\n",
      "      )\n",
      "    )\n",
      "    (1): DownBlock(\n",
      "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): DoubleConvBlock(\n",
      "        (0): ConvBlock(\n",
      "          (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): LeakyReLU(negative_slope=0.1)\n",
      "        )\n",
      "        (1): ConvBlock(\n",
      "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): LeakyReLU(negative_slope=0.1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): DownBlock(\n",
      "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): DoubleConvBlock(\n",
      "        (0): ConvBlock(\n",
      "          (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): LeakyReLU(negative_slope=0.1)\n",
      "        )\n",
      "        (1): ConvBlock(\n",
      "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): LeakyReLU(negative_slope=0.1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): DownBlock(\n",
      "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): DoubleConvBlock(\n",
      "        (0): ConvBlock(\n",
      "          (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): LeakyReLU(negative_slope=0.1)\n",
      "        )\n",
      "        (1): ConvBlock(\n",
      "          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): LeakyReLU(negative_slope=0.1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (1): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (2): Flatten(start_dim=1, end_dim=-1)\n",
      "  (3): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch_tools import SimpleConvNet2d\n",
    "from torch.cuda import is_available\n",
    "\n",
    "DEVICE = \"cuda\" if is_available() else \"cpu\"\n",
    "\n",
    "model = SimpleConvNet2d(in_chans=3, out_feats=10).to(DEVICE)\n",
    "print(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we have instantiated a resnet18, with ImageNet-pretrained weights and applied a dropout to the classification layer.\n",
    "\n",
    "Now, lets set up a loss function and optimiser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import BCELoss\n",
    "from torch.optim import Adam\n",
    "\n",
    "loss_func = BCELoss(reduction=\"sum\")\n",
    "optimiser = Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, the training and validation loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import no_grad\n",
    "\n",
    "\n",
    "def train_one_epoch(\n",
    "    train_loader: DataLoader,\n",
    "    loss_func: BCELoss,\n",
    "    optimiser: Adam,\n",
    "    model: SimpleConvNet2d,\n",
    "):\n",
    "    \"\"\"Train the model for a single epoch.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_loader : DataLoader\n",
    "        A data loader supplying the training data.\n",
    "    loss_func : BCELoss\n",
    "        The loss function.\n",
    "    optimiser : Adam\n",
    "        Optimiser to fit the model with.\n",
    "    model : SimpleConvNet2d\n",
    "        Classification model.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    mean_loss : float\n",
    "        Mean loss per item.\n",
    "    accuracy : float\n",
    "        Fraction of inputs correctly classified.\n",
    "\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    for batch, targets in train_loader:\n",
    "        batch, targets = batch.to(DEVICE), targets.to(DEVICE)\n",
    "\n",
    "        optimiser.zero_grad()\n",
    "\n",
    "        preds = model(batch).softmax(dim=1)\n",
    "\n",
    "        loss = loss_func(preds, targets)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimiser.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        correct += (\n",
    "            (preds.argmax(dim=1).detach().cpu() == targets.cpu().argmax(dim=1)).sum().item()\n",
    "        )\n",
    "\n",
    "    mean_loss = running_loss / len(train_loader.dataset)\n",
    "    accuracy = correct / len(train_loader.dataset)\n",
    "\n",
    "    return mean_loss, accuracy\n",
    "\n",
    "\n",
    "@no_grad()\n",
    "def validate_one_epoch(\n",
    "    valid_loader: DataLoader,\n",
    "    loss_func: BCELoss,\n",
    "    model: SimpleConvNet2d,\n",
    "):\n",
    "    \"\"\"Validate the model for a single epoch.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    valid_loader : DataLoader\n",
    "        Validation data loader.\n",
    "    loss_func : BCELoss\n",
    "        Loss function\n",
    "    model : SimpleConvNet2d\n",
    "        Classification model.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    mean_loss : float\n",
    "        The mean loss per item.\n",
    "    accuracy : float\n",
    "        The fraction of correctly classified inputs.\n",
    "\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    for batch, targets in valid_loader:\n",
    "        batch, targets = batch.to(DEVICE), targets.to(DEVICE)\n",
    "\n",
    "        preds = model(batch).softmax(dim=1)\n",
    "\n",
    "        loss = loss_func(preds, targets)\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        correct += (\n",
    "            (preds.argmax(dim=1).detach().cpu() == targets.argmax(dim=1).cpu()).sum().item()\n",
    "        )\n",
    "\n",
    "    mean_loss = running_loss / len(valid_loader.dataset)\n",
    "    accuracy = correct / len(valid_loader.dataset)\n",
    "\n",
    "    return mean_loss, accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train the model.\n",
    "\n",
    "Note, for the first four training epochs we set the frozen encoder argument to ``True``, and then for the final two epochs we set it to ``False``. Take a look at the plots and see how this switching affects the performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "num_epochs = 6\n",
    "\n",
    "training_loss, validation_loss = [], []\n",
    "training_acc, validation_acc = [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train_one_epoch(\n",
    "        train_loader,\n",
    "        loss_func,\n",
    "        optimiser,\n",
    "        model,\n",
    "    )\n",
    "    valid_loss, valid_acc = validate_one_epoch(valid_loader, loss_func, model)\n",
    "\n",
    "    training_loss.append(train_loss)\n",
    "    training_acc.append(train_acc)\n",
    "\n",
    "    validation_loss.append(valid_loss)\n",
    "    validation_acc.append(valid_acc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot some simple performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2)\n",
    "\n",
    "axes[0].plot(training_loss, \"-o\", label=\"Train\")\n",
    "axes[0].plot(validation_loss, \"-o\", label=\"Valid\")\n",
    "\n",
    "axes[1].plot(training_acc, \"-o\", label=\"Train\")\n",
    "axes[1].plot(validation_acc, \"-o\", label=\"Valid\")\n",
    "\n",
    "\n",
    "for axis in axes.ravel():\n",
    "    axis.set_xlim(left=0, right=len(training_loss) - 1)\n",
    "    axis.set_xlabel(\"Epoch\")\n",
    "\n",
    "axes[0].set_ylim(bottom=0.0, top=2.0)\n",
    "axes[1].set_ylim(bottom=0.5, top=1.0)\n",
    "\n",
    "for axis in axes.ravel():\n",
    "    axis.plot(\n",
    "        [freeze_epochs - 1] * 2,\n",
    "        axis.get_ylim(),\n",
    "        \"--k\",\n",
    "        label=\"Unfreeze encoder\",\n",
    "    )\n",
    "    axis.set_aspect(\n",
    "        (axis.get_xlim()[1] - axis.get_xlim()[0])\n",
    "        / (axis.get_ylim()[1] - axis.get_ylim()[0]),\n",
    "    )\n",
    "    axis.legend(fontsize=9)\n",
    "\n",
    "axes[0].set_ylabel(\"Loss\")\n",
    "axes[1].set_ylabel(\"Classification accuracy\")\n",
    "\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the sudden boost in performance when we *unfreeze* the encoder. You might ask what was the point in freezing the encoder if the performance is suddenly boosted by unfreezing it. The short answer is that it really depends on the problem at hand, and training with the encoder frozen is vastly computationally cheaper than fitting the entire model.\n",
    "\n",
    "You might also consider that loading the encoder with pretrained weights, while the fully connected block is loaded with randomly initialised weights, is a less effective way of capitalising on the pretained weights in comparison to freezing the encoder (at least initially).\n",
    "\n",
    "For other problems, you may wish to pretrain the encoder yourself, so it is useful to have this functionality built in."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-tools-demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ca0431213683a4cf4fa8a04c822ab65ef03858b4260bcbbe63868c9f59e4f0b5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
